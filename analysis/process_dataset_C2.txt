
python run.py --do_train --per_device_train_batch_size 32 --task nli --dataset ./multinli_1.0_train_formatted.jsonl --model ./trained_SNLI_model_epochs_3 --output_dir ./trained_SNLI_model_epochs_3_MNLI_epochs_1  --num_train_epochs 1.0

PS C:\Users\kenta\Desktop\NLP\fp-dataset-artifcats-main_2> python run.py --do_train --per_device_train_batch_size 32 --task nli --dataset ./multinli_1.0_train_formatted.jsonl --model ./trained_SNLI_model_epochs_3 --output_dir ./trained_SNLI_model_epochs_3_MNLI_epochs_1  --num_train_epochs 1.0
>>
Preprocessing data... (this takes a little bit, should only happen once per dataset)
Map (num_proc=2): 100%|█████████████████████████████████████████████████████████████████████████████████████| 392702/392702 [00:33<00:00, 11822.67 examples/s]
C:\Users\kenta\Desktop\NLP\fp-dataset-artifcats-main_2\run.py:171: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = trainer_class(
{'loss': 0.6698, 'grad_norm': 8.847766876220703, 'learning_rate': 4.7962842242503265e-05, 'epoch': 0.04}
{'loss': 0.6395, 'grad_norm': 5.6731719970703125, 'learning_rate': 4.592568448500652e-05, 'epoch': 0.08}
{'loss': 0.6203, 'grad_norm': 7.7936177253723145, 'learning_rate': 4.3888526727509784e-05, 'epoch': 0.12}
{'loss': 0.6085, 'grad_norm': 6.69145393371582, 'learning_rate': 4.185136897001304e-05, 'epoch': 0.16}
{'loss': 0.6006, 'grad_norm': 7.994510173797607, 'learning_rate': 3.98142112125163e-05, 'epoch': 0.2}
{'loss': 0.5817, 'grad_norm': 6.436513900756836, 'learning_rate': 3.777705345501956e-05, 'epoch': 0.24}
{'loss': 0.5816, 'grad_norm': 9.509881973266602, 'learning_rate': 3.5739895697522816e-05, 'epoch': 0.29}
{'loss': 0.5642, 'grad_norm': 8.258749008178711, 'learning_rate': 3.370273794002607e-05, 'epoch': 0.33}
{'loss': 0.5682, 'grad_norm': 5.226851940155029, 'learning_rate': 3.1665580182529335e-05, 'epoch': 0.37}
{'loss': 0.5561, 'grad_norm': 6.845906734466553, 'learning_rate': 2.9628422425032598e-05, 'epoch': 0.41}
{'loss': 0.5483, 'grad_norm': 6.693856239318848, 'learning_rate': 2.7591264667535854e-05, 'epoch': 0.45}
{'loss': 0.5494, 'grad_norm': 4.448433876037598, 'learning_rate': 2.5554106910039117e-05, 'epoch': 0.49}
{'loss': 0.5475, 'grad_norm': 5.234847068786621, 'learning_rate': 2.3516949152542376e-05, 'epoch': 0.53}
{'loss': 0.5337, 'grad_norm': 4.209057331085205, 'learning_rate': 2.1479791395045636e-05, 'epoch': 0.57}
{'loss': 0.5313, 'grad_norm': 5.558631420135498, 'learning_rate': 1.944263363754889e-05, 'epoch': 0.61}
{'loss': 0.5344, 'grad_norm': 4.307429313659668, 'learning_rate': 1.740547588005215e-05, 'epoch': 0.65}
{'loss': 0.525, 'grad_norm': 5.490296363830566, 'learning_rate': 1.536831812255541e-05, 'epoch': 0.69}
{'loss': 0.5298, 'grad_norm': 5.046754837036133, 'learning_rate': 1.333116036505867e-05, 'epoch': 0.73}
{'loss': 0.5144, 'grad_norm': 8.422515869140625, 'learning_rate': 1.1294002607561931e-05, 'epoch': 0.77}
{'loss': 0.5152, 'grad_norm': 7.001948833465576, 'learning_rate': 9.25684485006519e-06, 'epoch': 0.81}
{'loss': 0.5142, 'grad_norm': 11.641801834106445, 'learning_rate': 7.219687092568449e-06, 'epoch': 0.86}
{'loss': 0.5153, 'grad_norm': 5.988468170166016, 'learning_rate': 5.182529335071708e-06, 'epoch': 0.9}
{'loss': 0.5088, 'grad_norm': 7.7681684494018555, 'learning_rate': 3.1453715775749674e-06, 'epoch': 0.94}
{'loss': 0.5063, 'grad_norm': 6.404898166656494, 'learning_rate': 1.108213820078227e-06, 'epoch': 0.98}
{'train_runtime': 1455.9908, 'train_samples_per_second': 269.715, 'train_steps_per_second': 8.429, 'train_loss': 0.5559880892021248, 'epoch': 1.0}
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 12272/12272 [24:15<00:00,  8.43it/s]
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 49088/49088 [11:55<00:00, 68.62it/s]
PredictionOutput(predictions=array([[ 1.3462234 ,  0.01665524, -1.817746  ],
       [ 1.9050744 , -1.0808153 , -0.96254236],
       [-0.0277427 ,  1.1755004 , -1.6016687 ],
       ...,
       [-0.5659141 ,  0.89675367, -0.50966835],
       [-1.3576287 ,  2.455447  , -1.7597038 ],
       [-2.5467846 ,  2.0202675 ,  0.23380032]], dtype=float32), label_ids=array([1, 0, 0, ..., 0, 1, 1], dtype=int64), metrics={'test_loss': 0.42393171787261963, 'test_accuracy': 0.8363237380981445, 'test_runtime': 715.4237, 'test_samples_per_second': 548.908, 'test_steps_per_second': 68.614})
wrong_indices: [     0      2      4 ... 392694 392696 392699]
len(wrong_indices): 64276
type(wrong_indices): <class 'numpy.ndarray'>
type(train_dataset): <class 'datasets.arrow_dataset.Dataset'>
PS C:\Users\kenta\Desktop\NLP\fp-dataset-artifcats-main_2>
>>

last learning rate 1.108213820078227e-06

python run.py --do_eval --per_device_train_batch_size 32 --task nli --dataset ./breaking_nli_modified.jsonl --model ./trained_SNLI_model_epochs_3_MNLI_epochs_1 --output_dir  ./eval_trained_SNLI_model_epochs_3_MNLI_epochs_1/

{"eval_loss": 0.1745166927576065, "eval_model_preparation_time": 0.0025, "eval_accuracy": 0.9491028785705566, "eval_runtime": 14.898, "eval_samples_per_second": 549.941, "eval_steps_per_second": 68.801}

python run.py --do_train --per_device_train_batch_size 32 --task nli --dataset ./multinli_1.0_train_formatted.jsonl --model ./trained_SNLI_model_epochs_3_MNLI_epochs_1 --output_dir ./trained_SNLI_model_epochs_3_MNLI_epochs_2  --num_train_epochs 1.0  --learning_rate 1.108213820078227e-06


PS C:\Users\kenta\Desktop\NLP\fp-dataset-artifcats-main_2> python run.py --do_train --per_device_train_batch_size 32 --task nli --dataset ./multinli_1.0_train_formatted.jsonl --model ./trained_SNLI_model_epochs_3_MNLI_epochs_1 --output_dir ./trained_SNLI_model_epochs_3_MNLI_epochs_2  --num_train_epochs 1.0  --learning_rate 1.108213820078227e-06
>>
Preprocessing data... (this takes a little bit, should only happen once per dataset)
Map (num_proc=2): 100%|█████████████████████████████████████████████████████████████████████████████████████| 392702/392702 [00:35<00:00, 10951.61 examples/s]
C:\Users\kenta\Desktop\NLP\fp-dataset-artifcats-main_2\run.py:171: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = trainer_class(
{'loss': 0.3959, 'grad_norm': 5.065678119659424, 'learning_rate': 1.063061692467478e-06, 'epoch': 0.04}
{'loss': 0.3938, 'grad_norm': 7.718425750732422, 'learning_rate': 1.0179095648567287e-06, 'epoch': 0.08}
{'loss': 0.3718, 'grad_norm': 8.01772403717041, 'learning_rate': 9.727574372459797e-07, 'epoch': 0.12}
{'loss': 0.3701, 'grad_norm': 5.503514766693115, 'learning_rate': 9.276053096352305e-07, 'epoch': 0.16}
{'loss': 0.3579, 'grad_norm': 6.4132843017578125, 'learning_rate': 8.824531820244814e-07, 'epoch': 0.2}
{'loss': 0.3456, 'grad_norm': 3.06795597076416, 'learning_rate': 8.373010544137321e-07, 'epoch': 0.24}
{'loss': 0.3508, 'grad_norm': 5.478929042816162, 'learning_rate': 7.92148926802983e-07, 'epoch': 0.29}
{'loss': 0.3336, 'grad_norm': 6.6454243659973145, 'learning_rate': 7.469967991922339e-07, 'epoch': 0.33}
{'loss': 0.3411, 'grad_norm': 7.200390815734863, 'learning_rate': 7.018446715814848e-07, 'epoch': 0.37}
{'loss': 0.3379, 'grad_norm': 6.737461566925049, 'learning_rate': 6.566925439707356e-07, 'epoch': 0.41}
{'loss': 0.3359, 'grad_norm': 8.11268138885498, 'learning_rate': 6.115404163599864e-07, 'epoch': 0.45}
{'loss': 0.3354, 'grad_norm': 9.16856861114502, 'learning_rate': 5.663882887492373e-07, 'epoch': 0.49}
{'loss': 0.3423, 'grad_norm': 4.222217559814453, 'learning_rate': 5.212361611384882e-07, 'epoch': 0.53}
{'loss': 0.3427, 'grad_norm': 7.163985729217529, 'learning_rate': 4.7608403352773904e-07, 'epoch': 0.57}
{'loss': 0.3487, 'grad_norm': 8.353156089782715, 'learning_rate': 4.3093190591698986e-07, 'epoch': 0.61}
{'loss': 0.358, 'grad_norm': 5.308055877685547, 'learning_rate': 3.8577977830624073e-07, 'epoch': 0.65}
{'loss': 0.3618, 'grad_norm': 6.8441162109375, 'learning_rate': 3.4062765069549155e-07, 'epoch': 0.69}
{'loss': 0.3874, 'grad_norm': 7.016338348388672, 'learning_rate': 2.954755230847424e-07, 'epoch': 0.73}
{'loss': 0.3851, 'grad_norm': 8.632010459899902, 'learning_rate': 2.503233954739933e-07, 'epoch': 0.77}
{'loss': 0.4039, 'grad_norm': 8.324457168579102, 'learning_rate': 2.0517126786324412e-07, 'epoch': 0.81}
{'loss': 0.4291, 'grad_norm': 12.039645195007324, 'learning_rate': 1.6001914025249497e-07, 'epoch': 0.86}
{'loss': 0.4532, 'grad_norm': 7.655817031860352, 'learning_rate': 1.1486701264174584e-07, 'epoch': 0.9}
{'loss': 0.4708, 'grad_norm': 10.596210479736328, 'learning_rate': 6.971488503099669e-08, 'epoch': 0.94}
{'loss': 0.4956, 'grad_norm': 10.31447696685791, 'learning_rate': 2.4562757420247537e-08, 'epoch': 0.98}
{'train_runtime': 1445.9291, 'train_samples_per_second': 271.591, 'train_steps_per_second': 8.487, 'train_loss': 0.38043199959447827, 'epoch': 1.0}
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 12272/12272 [24:05<00:00,  8.49it/s]
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 49088/49088 [11:56<00:00, 68.48it/s]
PredictionOutput(predictions=array([[ 1.5601889 ,  0.22730368, -2.3915784 ],
       [ 2.239274  , -1.3983228 , -0.92070395],
       [ 0.09648862,  1.4924228 , -2.2346492 ],
       ...,
       [-0.69312763,  1.3241422 , -0.9471565 ],
       [-1.558126  ,  3.1377304 , -2.5014837 ],
       [-2.8701673 ,  2.5882728 , -0.16280729]], dtype=float32), label_ids=array([1, 0, 0, ..., 0, 1, 1], dtype=int64), metrics={'test_loss': 0.4363434314727783, 'test_accuracy': 0.8388141393661499, 'test_runtime': 716.8357, 'test_samples_per_second': 547.827, 'test_steps_per_second': 68.479})
wrong_indices: [     0      2      4 ... 392694 392696 392699]
len(wrong_indices): 63298
type(wrong_indices): <class 'numpy.ndarray'>
type(train_dataset): <class 'datasets.arrow_dataset.Dataset'>
PS C:\Users\kenta\Desktop\NLP\fp-dataset-artifcats-main_2>
>>

last learning rate 2.4562757420247537e-08

python run.py --do_eval --per_device_train_batch_size 32 --task nli --dataset ./breaking_nli_modified.jsonl --model ./trained_SNLI_model_epochs_3_MNLI_epochs_2 --output_dir  ./eval_trained_SNLI_model_epochs_3_MNLI_epochs_2/

{"eval_loss": 0.18459880352020264, "eval_model_preparation_time": 0.002, "eval_accuracy": 0.947882354259491, "eval_runtime": 15.1353, "eval_samples_per_second": 541.318, "eval_steps_per_second": 67.723}

python run.py --do_train --per_device_train_batch_size 32 --task nli --dataset ./multinli_1.0_train_formatted.jsonl --model ./trained_SNLI_model_epochs_3_MNLI_epochs_2 --output_dir ./trained_SNLI_model_epochs_3_MNLI_epochs_3  --num_train_epochs 1.0  --learning_rate 2.4562757420247537e-08

PS C:\Users\kenta\Desktop\NLP\fp-dataset-artifcats-main_2> python run.py --do_train --per_device_train_batch_size 32 --task nli --dataset ./multinli_1.0_train_formatted.jsonl --model ./trained_SNLI_model_epochs_3_MNLI_epochs_2 --output_dir ./trained_SNLI_model_epochs_3_MNLI_epochs_3  --num_train_epochs 1.0  --learning_rate 2.4562757420247537e-08
>>
Preprocessing data... (this takes a little bit, should only happen once per dataset)
Map (num_proc=2): 100%|█████████████████████████████████████████████████████████████████████████████████████| 392702/392702 [00:32<00:00, 11973.96 examples/s]
C:\Users\kenta\Desktop\NLP\fp-dataset-artifcats-main_2\run.py:171: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = trainer_class(
{'loss': 0.3748, 'grad_norm': 6.1449503898620605, 'learning_rate': 2.356199318376418e-08, 'epoch': 0.04}
{'loss': 0.3737, 'grad_norm': 9.719173431396484, 'learning_rate': 2.256122894728082e-08, 'epoch': 0.08}
{'loss': 0.3534, 'grad_norm': 8.18494987487793, 'learning_rate': 2.1560464710797463e-08, 'epoch': 0.12}
{'loss': 0.3519, 'grad_norm': 5.659355163574219, 'learning_rate': 2.0559700474314105e-08, 'epoch': 0.16}
{'loss': 0.3399, 'grad_norm': 6.330198287963867, 'learning_rate': 1.9558936237830747e-08, 'epoch': 0.2}
{'loss': 0.329, 'grad_norm': 3.152564525604248, 'learning_rate': 1.855817200134739e-08, 'epoch': 0.24}
{'loss': 0.3346, 'grad_norm': 5.125274181365967, 'learning_rate': 1.7557407764864032e-08, 'epoch': 0.29}
{'loss': 0.3212, 'grad_norm': 6.054457664489746, 'learning_rate': 1.655664352838067e-08, 'epoch': 0.33}
{'loss': 0.3264, 'grad_norm': 6.783513069152832, 'learning_rate': 1.5555879291897316e-08, 'epoch': 0.37}
{'loss': 0.3249, 'grad_norm': 6.461134910583496, 'learning_rate': 1.4555115055413958e-08, 'epoch': 0.41}
{'loss': 0.325, 'grad_norm': 7.8042144775390625, 'learning_rate': 1.3554350818930599e-08, 'epoch': 0.45}
{'loss': 0.3267, 'grad_norm': 7.462625503540039, 'learning_rate': 1.2553586582447242e-08, 'epoch': 0.49}
{'loss': 0.3334, 'grad_norm': 3.7085084915161133, 'learning_rate': 1.1552822345963884e-08, 'epoch': 0.53}
{'loss': 0.3343, 'grad_norm': 6.079791069030762, 'learning_rate': 1.0552058109480527e-08, 'epoch': 0.57}
{'loss': 0.3403, 'grad_norm': 7.70441198348999, 'learning_rate': 9.551293872997169e-09, 'epoch': 0.61}
{'loss': 0.3506, 'grad_norm': 4.811441898345947, 'learning_rate': 8.55052963651381e-09, 'epoch': 0.65}
{'loss': 0.3546, 'grad_norm': 6.497406005859375, 'learning_rate': 7.549765400030451e-09, 'epoch': 0.69}
{'loss': 0.3804, 'grad_norm': 6.510363578796387, 'learning_rate': 6.549001163547094e-09, 'epoch': 0.73}
{'loss': 0.3801, 'grad_norm': 8.46602725982666, 'learning_rate': 5.548236927063736e-09, 'epoch': 0.77}
{'loss': 0.3988, 'grad_norm': 8.10606575012207, 'learning_rate': 4.5474726905803785e-09, 'epoch': 0.81}
{'loss': 0.4248, 'grad_norm': 11.884644508361816, 'learning_rate': 3.5467084540970206e-09, 'epoch': 0.86}
{'loss': 0.4504, 'grad_norm': 7.691428184509277, 'learning_rate': 2.5459442176136627e-09, 'epoch': 0.9}
{'loss': 0.4699, 'grad_norm': 10.611051559448242, 'learning_rate': 1.5451799811303046e-09, 'epoch': 0.94}
{'loss': 0.4969, 'grad_norm': 10.919121742248535, 'learning_rate': 5.444157446469467e-10, 'epoch': 0.98}
{'train_runtime': 1447.8645, 'train_samples_per_second': 271.228, 'train_steps_per_second': 8.476, 'train_loss': 0.3702147955067621, 'epoch': 1.0}
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 12272/12272 [24:07<00:00,  8.48it/s]
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 49088/49088 [12:00<00:00, 68.10it/s]
PredictionOutput(predictions=array([[ 1.584788  ,  0.26609397, -2.4783566 ],
       [ 2.2828262 , -1.407826  , -0.9624055 ],
       [ 0.03987344,  1.5983003 , -2.3135154 ],
       ...,
       [-0.7781088 ,  1.4285954 , -0.9910858 ],
       [-1.5932602 ,  3.187572  , -2.5296779 ],
       [-2.9372993 ,  2.6486409 , -0.16768496]], dtype=float32), label_ids=array([1, 0, 0, ..., 0, 1, 1], dtype=int64), metrics={'test_loss': 0.43866610527038574, 'test_accuracy': 0.8389083743095398, 'test_runtime': 720.8429, 'test_samples_per_second': 544.782, 'test_steps_per_second': 68.098})
wrong_indices: [     0      2      4 ... 392694 392696 392699]
len(wrong_indices): 63261
type(wrong_indices): <class 'numpy.ndarray'>
type(train_dataset): <class 'datasets.arrow_dataset.Dataset'>
PS C:\Users\kenta\Desktop\NLP\fp-dataset-artifcats-main_2>
>>

last learning rate 5.444157446469467e-10

python run.py --do_eval --per_device_train_batch_size 32 --task nli --dataset ./breaking_nli_modified.jsonl --model ./trained_SNLI_model_epochs_3_MNLI_epochs_3 --output_dir  ./eval_trained_SNLI_model_epochs_3_MNLI_epochs_3/

{"eval_loss": 0.186866894364357, "eval_model_preparation_time": 0.002, "eval_accuracy": 0.9473941326141357, "eval_runtime": 14.8189, "eval_samples_per_second": 552.875, "eval_steps_per_second": 69.168}

C:\Users\kenta\Desktop\NLP\fp-dataset-artifcats-main_2\pythonProject\.venv\Scripts\python.exe "C:\Users\kenta\Desktop\NLP\fp-dataset-artifcats-main_2\wrong indexes intersect.py" 
len(numbers_set_1): 64276
len(numbers_set_2): 63298
len(numbers_set_3): 63261
len(numbers_set_1_or_2): 68199
len(numbers_set_1_or_3): 68156
len(numbers_set_2_or_3): 63857
len(numbers_set_1_or_2_or_3): 68464
len(numbers_set_1_and_2): 59375
len(numbers_set_1_and_3): 59381
len(numbers_set_2_and_3): 62702
len(numbers_set_1_and_2_and_3): 59087

Process finished with exit code 0


python run.py --do_train --per_device_train_batch_size 32 --task nli --dataset ./multinli_1.0_train_formatted_wrong_examples_.jsonl --model ./trained_SNLI_model_epochs_3_MNLI_epochs_3 --output_dir ./trained_model_epochs_3_wrongex_epoch_1_lr_1e-7  --num_train_epochs 1.0 --learning_rate 1.0e-7

PS C:\Users\kenta\Desktop\NLP\fp-dataset-artifcats-main_2> python run.py --do_train --per_device_train_batch_size 32 --task nli --dataset ./multinli_1.0_train_formatted_wrong_examples_.jsonl --model ./trained_SNLI_model_epochs_3_MNLI_epochs_3 --output_dir ./trained_model_epochs_3_wrongex_epoch_1_lr_1e-7  --num_train_epochs 1.0 --learning_rate 1.0e-7
>>
Generating train split: 59087 examples [00:00, 781917.78 examples/s]
Preprocessing data... (this takes a little bit, should only happen once per dataset)
Map (num_proc=2): 100%|████████████████████████████████████████████████████████████████████████████████████████| 59087/59087 [00:11<00:00, 5234.63 examples/s]
C:\Users\kenta\Desktop\NLP\fp-dataset-artifcats-main_2\run.py:171: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = trainer_class(
{'loss': 1.9634, 'grad_norm': 16.294260025024414, 'learning_rate': 7.292907417433675e-08, 'epoch': 0.27}
{'loss': 1.8829, 'grad_norm': 15.567527770996094, 'learning_rate': 4.585814834867352e-08, 'epoch': 0.54}
{'loss': 1.8382, 'grad_norm': 14.702028274536133, 'learning_rate': 1.8787222523010286e-08, 'epoch': 0.81}
{'train_runtime': 218.6746, 'train_samples_per_second': 270.205, 'train_steps_per_second': 8.446, 'train_loss': 1.8796775004124087, 'epoch': 1.0}
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1847/1847 [03:38<00:00,  8.45it/s]
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 7386/7386 [01:45<00:00, 69.70it/s]
PredictionOutput(predictions=array([[ 1.138009  ,  0.25029922, -1.850805  ],
       [-0.01682172,  1.3094673 , -1.8178304 ],
       [ 1.9150105 , -0.3799719 , -2.044438  ],
       ...,
       [ 0.9770013 ,  0.73810714, -2.399407  ],
       [-3.28326   ,  1.2341268 ,  2.0367274 ],
       [-0.66598547,  1.1781251 , -0.78098136]], dtype=float32), label_ids=array([1, 0, 1, ..., 1, 1, 0], dtype=int64), metrics={'test_loss': 2.033389091491699, 'test_accuracy': 0.029600419104099274, 'test_runtime': 105.9895, 'test_samples_per_second': 557.48, 'test_steps_per_second': 69.686})
wrong_indices: [    0     1     2 ... 59084 59085 59086]
len(wrong_indices): 57338
type(wrong_indices): <class 'numpy.ndarray'>

python run.py --do_eval --per_device_train_batch_size 32 --task nli --dataset ./breaking_nli_modified.jsonl --model ./trained_model_epochs_3_wrongex_epoch_1_lr_1e-7 --output_dir  ./eval_trained_model_epochs_3_wrongex_epoch_1_lr_1e-7/

{"eval_loss": 0.17320893704891205, "eval_model_preparation_time": 0.002, "eval_accuracy": 0.9504454731941223, "eval_runtime": 14.9894, "eval_samples_per_second": 546.585, "eval_steps_per_second": 68.382}

python run.py --do_train --per_device_train_batch_size 32 --task nli --dataset ./multinli_1.0_train_formatted_wrong_examples_.jsonl --model ./trained_SNLI_model_epochs_3_MNLI_epochs_3 --output_dir ./trained_model_epochs_3_wrongex_epoch_1_lr_1e-6  --num_train_epochs 1.0 --learning_rate 1.0e-6

PS C:\Users\kenta\Desktop\NLP\fp-dataset-artifcats-main_2> python run.py --do_train --per_device_train_batch_size 32 --task nli --dataset ./multinli_1.0_train_formatted_wrong_examples_.jsonl --model ./trained_SNLI_model_epochs_3_MNLI_epochs_3 --output_dir ./trained_model_epochs_3_wrongex_epoch_1_lr_1e-6  --num_train_epochs 1.0 --learning_rate 1.0e-6
>>
Preprocessing data... (this takes a little bit, should only happen once per dataset)
C:\Users\kenta\Desktop\NLP\fp-dataset-artifcats-main_2\run.py:171: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = trainer_class(
{'loss': 1.5608, 'grad_norm': 6.982916355133057, 'learning_rate': 7.292907417433676e-07, 'epoch': 0.27}
{'loss': 1.1901, 'grad_norm': 3.720088005065918, 'learning_rate': 4.585814834867352e-07, 'epoch': 0.54}
{'loss': 1.1214, 'grad_norm': 3.273890733718872, 'learning_rate': 1.8787222523010285e-07, 'epoch': 0.81}
{'train_runtime': 217.5252, 'train_samples_per_second': 271.633, 'train_steps_per_second': 8.491, 'train_loss': 1.25542252916998, 'epoch': 1.0}
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1847/1847 [03:37<00:00,  8.49it/s]
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 7386/7386 [01:46<00:00, 69.29it/s]
PredictionOutput(predictions=array([[ 0.0284365 ,  0.09363807, -0.15973131],
       [-0.01945888,  0.22362287, -0.2821466 ],
       [ 0.04340718,  0.14166117, -0.22903243],
       ...,
       [ 0.26133835,  0.39203483, -0.8758709 ],
       [-0.95583177,  0.5376682 ,  0.3890146 ],
       [-0.05910249,  0.20467827, -0.20230468]], dtype=float32), label_ids=array([1, 0, 1, ..., 1, 1, 0], dtype=int64), metrics={'test_loss': 1.1162198781967163, 'test_accuracy': 0.31348690390586853, 'test_runtime': 106.6068, 'test_samples_per_second': 554.251, 'test_steps_per_second': 69.283})
wrong_indices: [    1     3     4 ... 59080 59083 59086]
len(wrong_indices): 40564
type(wrong_indices): <class 'numpy.ndarray'>
type(train_dataset): <class 'datasets.arrow_dataset.Dataset'>
PS C:\Users\kenta\Desktop\NLP\fp-dataset-artifcats-main_2>
>>

python run.py --do_eval --per_device_train_batch_size 32 --task nli --dataset ./breaking_nli_modified.jsonl --model ./trained_model_epochs_3_wrongex_epoch_1_lr_1e-6 --output_dir  ./eval_trained_model_epochs_3_wrongex_epoch_1_lr_1e-6/

{"eval_loss": 0.49406832456588745, "eval_model_preparation_time": 0.002, "eval_accuracy": 0.8950323462486267, "eval_runtime": 15.0917, "eval_samples_per_second": 542.882, "eval_steps_per_second": 67.918}

python run.py --do_train --per_device_train_batch_size 32 --task nli --dataset ./multinli_1.0_train_formatted_wrong_examples_.jsonl --model ./trained_SNLI_model_epochs_3_MNLI_epochs_3 --output_dir ./trained_model_epochs_3_wrongex_epoch_1_lr_1e-8  --num_train_epochs 1.0 --learning_rate 1.0e-8

PS C:\Users\kenta\Desktop\NLP\fp-dataset-artifcats-main_2> python run.py --do_train --per_device_train_batch_size 32 --task nli --dataset ./multinli_1.0_train_formatted_wrong_examples_.jsonl --model ./trained_SNLI_model_epochs_3_MNLI_epochs_3 --output_dir ./trained_model_epochs_3_wrongex_epoch_1_lr_1e-8  --num_train_epochs 1.0 --learning_rate 1.0e-8
>>
Preprocessing data... (this takes a little bit, should only happen once per dataset)
C:\Users\kenta\Desktop\NLP\fp-dataset-artifcats-main_2\run.py:171: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = trainer_class(
{'loss': 2.0164, 'grad_norm': 16.86835289001465, 'learning_rate': 7.2929074174336766e-09, 'epoch': 0.27}
{'loss': 2.0157, 'grad_norm': 17.64473533630371, 'learning_rate': 4.585814834867352e-09, 'epoch': 0.54}
{'loss': 2.0199, 'grad_norm': 16.65092658996582, 'learning_rate': 1.8787222523010285e-09, 'epoch': 0.81}
{'train_runtime': 218.1063, 'train_samples_per_second': 270.909, 'train_steps_per_second': 8.468, 'train_loss': 2.01751973348885, 'epoch': 1.0}
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1847/1847 [03:38<00:00,  8.47it/s]
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 7386/7386 [01:47<00:00, 68.55it/s]
PredictionOutput(predictions=array([[ 1.5426667 ,  0.26855913, -2.4254339 ],
       [ 0.03276501,  1.5753858 , -2.2704136 ],
       [ 2.3276813 , -0.5252851 , -2.383672  ],
       ...,
       [ 1.1235087 ,  0.7588769 , -2.637102  ],
       [-3.529112  ,  1.2663196 ,  2.2550604 ],
       [-0.77097434,  1.4077517 , -0.9708735 ]], dtype=float32), label_ids=array([1, 0, 1, ..., 1, 1, 0], dtype=int64), metrics={'test_loss': 2.280472755432129, 'test_accuracy': 0.001743192202411592, 'test_runtime': 107.7684, 'test_samples_per_second': 548.278, 'test_steps_per_second': 68.536})
wrong_indices: [    0     1     2 ... 59084 59085 59086]
len(wrong_indices): 58984
type(wrong_indices): <class 'numpy.ndarray'>
type(train_dataset): <class 'datasets.arrow_dataset.Dataset'>
PS C:\Users\kenta\Desktop\NLP\fp-dataset-artifcats-main_2>
>>

python run.py --do_eval --per_device_train_batch_size 32 --task nli --dataset ./breaking_nli_modified.jsonl --model ./trained_model_epochs_3_wrongex_epoch_1_lr_1e-8 --output_dir  ./eval_trained_model_epochs_3_wrongex_epoch_1_lr_1e-8/

{"eval_loss": 0.18546243011951447, "eval_model_preparation_time": 0.002, "eval_accuracy": 0.947638213634491, "eval_runtime": 14.8226, "eval_samples_per_second": 552.735, "eval_steps_per_second": 69.151}

python run.py --do_train --per_device_train_batch_size 32 --task nli --dataset ./multinli_1.0_train_formatted_wrong_examples_.jsonl --model ./trained_SNLI_model_epochs_3_MNLI_epochs_3 --output_dir ./trained_model_epochs_3_wrongex_epoch_1_lr_2e-7  --num_train_epochs 1.0 --learning_rate 2.0e-7

PS C:\Users\kenta\Desktop\NLP\fp-dataset-artifcats-main_2> python run.py --do_train --per_device_train_batch_size 32 --task nli --dataset ./multinli_1.0_train_formatted_wrong_examples_.jsonl --model ./trained_SNLI_model_epochs_3_MNLI_epochs_3 --output_dir ./trained_model_epochs_3_wrongex_epoch_1_lr_2e-7  --num_train_epochs 1.0 --learning_rate 2.0e-7
>>
Preprocessing data... (this takes a little bit, should only happen once per dataset)
C:\Users\kenta\Desktop\NLP\fp-dataset-artifcats-main_2\run.py:171: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = trainer_class(
{'loss': 1.9072, 'grad_norm': 16.061954498291016, 'learning_rate': 1.458581483486735e-07, 'epoch': 0.27}
{'loss': 1.749, 'grad_norm': 13.337301254272461, 'learning_rate': 9.171629669734705e-08, 'epoch': 0.54}
{'loss': 1.6627, 'grad_norm': 12.850935935974121, 'learning_rate': 3.757444504602057e-08, 'epoch': 0.81}
{'train_runtime': 217.6903, 'train_samples_per_second': 271.427, 'train_steps_per_second': 8.485, 'train_loss': 1.7446209422691357, 'epoch': 1.0}
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1847/1847 [03:37<00:00,  8.48it/s]
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 7386/7386 [01:47<00:00, 68.71it/s]
PredictionOutput(predictions=array([[ 0.7867456 ,  0.18652171, -1.2768831 ],
       [-0.03169807,  1.0060222 , -1.3631316 ],
       [ 1.4544911 , -0.24428993, -1.6150457 ],
       ...,
       [ 0.8499883 ,  0.6882934 , -2.144889  ],
       [-2.9855425 ,  1.1655601 ,  1.8062534 ],
       [-0.5114527 ,  0.9178727 , -0.6106527 ]], dtype=float32), label_ids=array([1, 0, 1, ..., 1, 1, 0], dtype=int64), metrics={'test_loss': 1.7966011762619019, 'test_accuracy': 0.06256875395774841, 'test_runtime': 107.5157, 'test_samples_per_second': 549.566, 'test_steps_per_second': 68.697})
wrong_indices: [    0     1     2 ... 59084 59085 59086]
len(wrong_indices): 55390
type(wrong_indices): <class 'numpy.ndarray'>
type(train_dataset): <class 'datasets.arrow_dataset.Dataset'>
PS C:\Users\kenta\Desktop\NLP\fp-dataset-artifcats-main_2>
>>

python run.py --do_eval --per_device_train_batch_size 32 --task nli --dataset ./breaking_nli_modified.jsonl --model ./trained_model_epochs_3_wrongex_epoch_1_lr_2e-7 --output_dir  ./eval_trained_model_epochs_3_wrongex_epoch_1_lr_2e-7/

{"eval_loss": 0.16656310856342316, "eval_model_preparation_time": 0.002, "eval_accuracy": 0.9503234624862671, "eval_runtime": 13.8837, "eval_samples_per_second": 590.117, "eval_steps_per_second": 73.828}

python run.py --do_train --per_device_train_batch_size 32 --task nli --dataset ./multinli_1.0_train_formatted_wrong_examples_.jsonl --model ./trained_SNLI_model_epochs_3_MNLI_epochs_3 --output_dir ./trained_model_epochs_3_wrongex_epoch_1_lr_3e-7  --num_train_epochs 1.0 --learning_rate 3.0e-7

PS C:\Users\kenta\Desktop\NLP\fp-dataset-artifcats-main_2> python run.py --do_train --per_device_train_batch_size 32 --task nli --dataset ./multinli_1.0_train_formatted_wrong_examples_.jsonl --model ./trained_SNLI_model_epochs_3_MNLI_epochs_3 --output_dir ./trained_model_epochs_3_wrongex_epoch_1_lr_3e-7  --num_train_epochs 1.0 --learning_rate 3.0e-7
>>
Preprocessing data... (this takes a little bit, should only happen once per dataset)
C:\Users\kenta\Desktop\NLP\fp-dataset-artifcats-main_2\run.py:171: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = trainer_class(
{'loss': 1.8536, 'grad_norm': 15.783985137939453, 'learning_rate': 2.1878722252301028e-07, 'epoch': 0.27}
{'loss': 1.63, 'grad_norm': 11.488740921020508, 'learning_rate': 1.3757444504602056e-07, 'epoch': 0.54}
{'loss': 1.5165, 'grad_norm': 11.347091674804688, 'learning_rate': 5.636166756903085e-08, 'epoch': 0.81}
{'train_runtime': 218.2924, 'train_samples_per_second': 270.678, 'train_steps_per_second': 8.461, 'train_loss': 1.6293129747986599, 'epoch': 1.0}
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1847/1847 [03:38<00:00,  8.46it/s]
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 7386/7386 [01:47<00:00, 68.46it/s]
PredictionOutput(predictions=array([[ 0.54222786,  0.12123969, -0.85206157],
       [-0.02814607,  0.74565357, -0.99917084],
       [ 1.0368122 , -0.13426413, -1.1933426 ],
       ...,
       [ 0.745376  ,  0.62791646, -1.9060364 ],
       [-2.6716568 ,  1.0801393 ,  1.5761302 ],
       [-0.35979164,  0.6875578 , -0.4808472 ]], dtype=float32), label_ids=array([1, 0, 1, ..., 1, 1, 0], dtype=int64), metrics={'test_loss': 1.6017413139343262, 'test_accuracy': 0.09281229227781296, 'test_runtime': 107.9083, 'test_samples_per_second': 547.567, 'test_steps_per_second': 68.447})
wrong_indices: [    0     1     2 ... 59084 59085 59086]
len(wrong_indices): 53603
type(wrong_indices): <class 'numpy.ndarray'>
type(train_dataset): <class 'datasets.arrow_dataset.Dataset'>
PS C:\Users\kenta\Desktop\NLP\fp-dataset-artifcats-main_2>
>>

python run.py --do_eval --per_device_train_batch_size 32 --task nli --dataset ./breaking_nli_modified.jsonl --model ./trained_model_epochs_3_wrongex_epoch_1_lr_3e-7 --output_dir  ./eval_trained_model_epochs_3_wrongex_epoch_1_lr_3e-7/

{"eval_loss": 0.1691507250070572, "eval_model_preparation_time": 0.002, "eval_accuracy": 0.9495911002159119, "eval_runtime": 14.8385, "eval_samples_per_second": 552.147, "eval_steps_per_second": 69.077}

python run.py --do_train --per_device_train_batch_size 32 --task nli --dataset ./multinli_1.0_train_formatted_wrong_examples_.jsonl --model ./trained_SNLI_model_epochs_3_MNLI_epochs_3 --output_dir ./trained_model_epochs_3_wrongex_epoch_1_lr_25e-7  --num_train_epochs 1.0 --learning_rate 2.5e-7

PS C:\Users\kenta\Desktop\NLP\fp-dataset-artifcats-main_2> python run.py --do_train --per_device_train_batch_size 32 --task nli --dataset ./multinli_1.0_train_formatted_wrong_examples_.jsonl --model ./trained_SNLI_model_epochs_3_MNLI_epochs_3 --output_dir ./trained_model_epochs_3_wrongex_epoch_1_lr_25e-7  --num_train_epochs 1.0 --learning_rate 2.5e-7
>>
Preprocessing data... (this takes a little bit, should only happen once per dataset)
C:\Users\kenta\Desktop\NLP\fp-dataset-artifcats-main_2\run.py:171: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = trainer_class(
{'loss': 1.88, 'grad_norm': 15.959403038024902, 'learning_rate': 1.823226854358419e-07, 'epoch': 0.27}
{'loss': 1.6876, 'grad_norm': 12.36227798461914, 'learning_rate': 1.146453708716838e-07, 'epoch': 0.54}
{'loss': 1.5859, 'grad_norm': 12.056815147399902, 'learning_rate': 4.696805630752571e-08, 'epoch': 0.81}
{'train_runtime': 217.9331, 'train_samples_per_second': 271.124, 'train_steps_per_second': 8.475, 'train_loss': 1.6844496301011775, 'epoch': 1.0}
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1847/1847 [03:37<00:00,  8.48it/s]
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 7386/7386 [01:47<00:00, 68.65it/s]
PredictionOutput(predictions=array([[ 0.65212697,  0.1523014 , -1.0442151 ],
       [-0.03123404,  0.86840165, -1.1683791 ],
       [ 1.2370707 , -0.18598269, -1.3985983 ],
       ...,
       [ 0.79556006,  0.6585409 , -2.0230982 ],
       [-2.8301044 ,  1.1242051 ,  1.6915346 ],
       [-0.4331423 ,  0.79723364, -0.5408707 ]], dtype=float32), label_ids=array([1, 0, 1, ..., 1, 1, 0], dtype=int64), metrics={'test_loss': 1.6937941312789917, 'test_accuracy': 0.07761436700820923, 'test_runtime': 107.5998, 'test_samples_per_second': 549.137, 'test_steps_per_second': 68.643})
wrong_indices: [    0     1     2 ... 59084 59085 59086]
len(wrong_indices): 54501
type(wrong_indices): <class 'numpy.ndarray'>
type(train_dataset): <class 'datasets.arrow_dataset.Dataset'>
PS C:\Users\kenta\Desktop\NLP\fp-dataset-artifcats-main_2>
>>

python run.py --do_eval --per_device_train_batch_size 32 --task nli --dataset ./breaking_nli_modified.jsonl --model ./trained_model_epochs_3_wrongex_epoch_1_lr_25e-7 --output_dir  ./eval_trained_model_epochs_3_wrongex_epoch_1_lr_25e-7/

{"eval_loss": 0.1665707379579544, "eval_model_preparation_time": 0.002, "eval_accuracy": 0.9499572515487671, "eval_runtime": 14.9473, "eval_samples_per_second": 548.124, "eval_steps_per_second": 68.574}

python run.py --do_train --per_device_train_batch_size 32 --task nli --dataset ./multinli_1.0_train_formatted_wrong_examples_.jsonl --model ./trained_SNLI_model_epochs_3_MNLI_epochs_3 --output_dir ./trained_model_epochs_3_wrongex_epoch_1_lr_15e-7  --num_train_epochs 1.0 --learning_rate 1.5e-7

PS C:\Users\kenta\Desktop\NLP\fp-dataset-artifcats-main_2> python run.py --do_train --per_device_train_batch_size 32 --task nli --dataset ./multinli_1.0_train_formatted_wrong_examples_.jsonl --model ./trained_SNLI_model_epochs_3_MNLI_epochs_3 --output_dir ./trained_model_epochs_3_wrongex_epoch_1_lr_15e-7  --num_train_epochs 1.0 --learning_rate 1.5e-7
>>
Preprocessing data... (this takes a little bit, should only happen once per dataset)
C:\Users\kenta\Desktop\NLP\fp-dataset-artifcats-main_2\run.py:171: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = trainer_class(
{'loss': 1.935, 'grad_norm': 16.149921417236328, 'learning_rate': 1.0939361126150514e-07, 'epoch': 0.27}
{'loss': 1.8141, 'grad_norm': 14.413407325744629, 'learning_rate': 6.878722252301028e-08, 'epoch': 0.54}
{'loss': 1.7469, 'grad_norm': 13.732390403747559, 'learning_rate': 2.8180833784515426e-08, 'epoch': 0.81}
{'train_runtime': 218.1752, 'train_samples_per_second': 270.824, 'train_steps_per_second': 8.466, 'train_loss': 1.8097382831005515, 'epoch': 1.0}
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1847/1847 [03:38<00:00,  8.47it/s]
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 7386/7386 [01:47<00:00, 69.01it/s]
PredictionOutput(predictions=array([[ 0.9486403 ,  0.22074711, -1.5484436 ],
       [-0.02777031,  1.1549811 , -1.581112  ],
       [ 1.6826314 , -0.3089142 , -1.8333466 ],
       ...,
       [ 0.90992033,  0.7154934 , -2.2705991 ],
       [-3.137037  ,  1.2028289 ,  1.9209841 ],
       [-0.59073377,  1.0462292 , -0.69061315]], dtype=float32), label_ids=array([1, 0, 1, ..., 1, 1, 0], dtype=int64), metrics={'test_loss': 1.9099303483963013, 'test_accuracy': 0.045966118574142456, 'test_runtime': 107.0403, 'test_samples_per_second': 552.007, 'test_steps_per_second': 69.002})
wrong_indices: [    0     1     2 ... 59084 59085 59086]
len(wrong_indices): 56371
type(wrong_indices): <class 'numpy.ndarray'>
type(train_dataset): <class 'datasets.arrow_dataset.Dataset'>
PS C:\Users\kenta\Desktop\NLP\fp-dataset-artifcats-main_2>
>>

python run.py --do_eval --per_device_train_batch_size 32 --task nli --dataset ./breaking_nli_modified.jsonl --model ./trained_model_epochs_3_wrongex_epoch_1_lr_15e-7 --output_dir  ./eval_trained_model_epochs_3_wrongex_epoch_1_lr_15e-7/

{"eval_loss": 0.16886141896247864, "eval_model_preparation_time": 0.001, "eval_accuracy": 0.9502013921737671, "eval_runtime": 15.1332, "eval_samples_per_second": 541.393, "eval_steps_per_second": 67.732}

python run.py --do_train --per_device_train_batch_size 32 --task nli --dataset ./multinli_1.0_train_formatted_wrong_examples_.jsonl --model ./trained_SNLI_model_epochs_3_MNLI_epochs_3 --output_dir ./trained_model_epochs_3_wrongex_epoch_1_lr_22e-7  --num_train_epochs 1.0 --learning_rate 2.2e-7

PS C:\Users\kenta\Desktop\NLP\fp-dataset-artifcats-main_2> python run.py --do_train --per_device_train_batch_size 32 --task nli --dataset ./multinli_1.0_train_formatted_wrong_examples_.jsonl --model ./trained_SNLI_model_epochs_3_MNLI_epochs_3 --output_dir ./trained_model_epochs_3_wrongex_epoch_1_lr_22e-7  --num_train_epochs 1.0 --learning_rate 2.2e-7
>>
Preprocessing data... (this takes a little bit, should only happen once per dataset)
C:\Users\kenta\Desktop\NLP\fp-dataset-artifcats-main_2\run.py:171: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = trainer_class(
{'loss': 1.8962, 'grad_norm': 16.026363372802734, 'learning_rate': 1.6044396318354087e-07, 'epoch': 0.27}
{'loss': 1.724, 'grad_norm': 12.93478775024414, 'learning_rate': 1.0088792636708176e-07, 'epoch': 0.54}
{'loss': 1.6311, 'grad_norm': 12.52294921875, 'learning_rate': 4.133188955062263e-08, 'epoch': 0.81}
{'train_runtime': 218.2179, 'train_samples_per_second': 270.771, 'train_steps_per_second': 8.464, 'train_loss': 1.719954048160869, 'epoch': 1.0}
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1847/1847 [03:38<00:00,  8.46it/s]
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 7386/7386 [01:48<00:00, 68.37it/s]
PredictionOutput(predictions=array([[ 0.7297268 ,  0.17262565, -1.1789323 ],
       [-0.03191924,  0.9493725 , -1.2822795 ],
       [ 1.3658653 , -0.2202034 , -1.5277147 ],
       ...,
       [ 0.827635  ,  0.6766006 , -2.0956612 ],
       [-2.9237845 ,  1.1494079 ,  1.7604314 ],
       [-0.47976413,  0.8684854 , -0.58153605]], dtype=float32), label_ids=array([1, 0, 1, ..., 1, 1, 0], dtype=int64), metrics={'test_loss': 1.7541999816894531, 'test_accuracy': 0.06864453852176666, 'test_runtime': 108.0541, 'test_samples_per_second': 546.828, 'test_steps_per_second': 68.355})
wrong_indices: [    0     1     2 ... 59084 59085 59086]
len(wrong_indices): 55031
type(wrong_indices): <class 'numpy.ndarray'>
type(train_dataset): <class 'datasets.arrow_dataset.Dataset'>
PS C:\Users\kenta\Desktop\NLP\fp-dataset-artifcats-main_2>
>>

python run.py --do_eval --per_device_train_batch_size 32 --task nli --dataset ./breaking_nli_modified.jsonl --model ./trained_model_epochs_3_wrongex_epoch_1_lr_22e-7 --output_dir  ./eval_trained_model_epochs_3_wrongex_epoch_1_lr_22e-7/

{"eval_loss": 0.16627538204193115, "eval_model_preparation_time": 0.002, "eval_accuracy": 0.9500793218612671, "eval_runtime": 15.0465, "eval_samples_per_second": 544.512, "eval_steps_per_second": 68.122}

python run.py --do_train --per_device_train_batch_size 32 --task nli --dataset ./multinli_1.0_train_formatted_wrong_examples_.jsonl --model ./trained_SNLI_model_epochs_3_MNLI_epochs_3 --output_dir ./trained_model_epochs_3_wrongex_epoch_1_lr_18e-7  --num_train_epochs 1.0 --learning_rate 1.8e-7

PS C:\Users\kenta\Desktop\NLP\fp-dataset-artifcats-main_2> python run.py --do_train --per_device_train_batch_size 32 --task nli --dataset ./multinli_1.0_train_formatted_wrong_examples_.jsonl --model ./trained_SNLI_model_epochs_3_MNLI_epochs_3 --output_dir ./trained_model_epochs_3_wrongex_epoch_1_lr_18e-7  --num_train_epochs 1.0 --learning_rate 1.8e-7
>>
Preprocessing data... (this takes a little bit, should only happen once per dataset)
C:\Users\kenta\Desktop\NLP\fp-dataset-artifcats-main_2\run.py:171: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = trainer_class(
{'loss': 1.9182, 'grad_norm': 16.094989776611328, 'learning_rate': 1.3127233351380617e-07, 'epoch': 0.27}
{'loss': 1.7746, 'grad_norm': 13.756233215332031, 'learning_rate': 8.254466702761233e-08, 'epoch': 0.54}
{'loss': 1.6955, 'grad_norm': 13.19295883178711, 'learning_rate': 3.3817000541418514e-08, 'epoch': 0.81}
{'train_runtime': 218.4027, 'train_samples_per_second': 270.542, 'train_steps_per_second': 8.457, 'train_loss': 1.7700806655429413, 'epoch': 1.0}
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1847/1847 [03:38<00:00,  8.46it/s]
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 7386/7386 [01:47<00:00, 68.50it/s]
PredictionOutput(predictions=array([[ 0.8481574 ,  0.20044078, -1.3811399 ],
       [-0.03078567,  1.0645167 , -1.4477292 ],
       [ 1.5448325 , -0.26938578, -1.7026633 ],
       ...,
       [ 0.87321645,  0.6995842 , -2.1947446 ],
       [-3.046683  ,  1.1810567 ,  1.852094  ],
       [-0.5433183 ,  0.9685061 , -0.641389  ]], dtype=float32), label_ids=array([1, 0, 1, ..., 1, 1, 0], dtype=int64), metrics={'test_loss': 1.840690016746521, 'test_accuracy': 0.05534212291240692, 'test_runtime': 107.8547, 'test_samples_per_second': 547.839, 'test_steps_per_second': 68.481})
wrong_indices: [    0     1     2 ... 59084 59085 59086]
len(wrong_indices): 55817
type(wrong_indices): <class 'numpy.ndarray'>
type(train_dataset): <class 'datasets.arrow_dataset.Dataset'>
PS C:\Users\kenta\Desktop\NLP\fp-dataset-artifcats-main_2>
>>

python run.py --do_eval --per_device_train_batch_size 32 --task nli --dataset ./breaking_nli_modified.jsonl --model ./trained_model_epochs_3_wrongex_epoch_1_lr_18e-7 --output_dir  ./eval_trained_model_epochs_3_wrongex_epoch_1_lr_18e-7/

{"eval_loss": 0.16721898317337036, "eval_model_preparation_time": 0.002, "eval_accuracy": 0.9504454731941223, "eval_runtime": 14.9461, "eval_samples_per_second": 548.171, "eval_steps_per_second": 68.58}

python run.py --do_train --per_device_train_batch_size 32 --task nli --dataset ./multinli_1.0_train_formatted_wrong_examples_.jsonl --model ./trained_SNLI_model_epochs_3_MNLI_epochs_3 --output_dir ./trained_model_epochs_3_wrongex_epoch_1_lr_17e-7  --num_train_epochs 1.0 --learning_rate 1.7e-7

PS C:\Users\kenta\Desktop\NLP\fp-dataset-artifcats-main_2> python run.py --do_train --per_device_train_batch_size 32 --task nli --dataset ./multinli_1.0_train_formatted_wrong_examples_.jsonl --model ./trained_SNLI_model_epochs_3_MNLI_epochs_3 --output_dir ./trained_model_epochs_3_wrongex_epoch_1_lr_17e-7  --num_train_epochs 1.0 --learning_rate 1.7e-7
>>
Preprocessing data... (this takes a little bit, should only happen once per dataset)
C:\Users\kenta\Desktop\NLP\fp-dataset-artifcats-main_2\run.py:171: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = trainer_class(
{'loss': 1.9238, 'grad_norm': 16.111928939819336, 'learning_rate': 1.2397942609637248e-07, 'epoch': 0.27}
{'loss': 1.7876, 'grad_norm': 13.971611022949219, 'learning_rate': 7.795885219274498e-08, 'epoch': 0.54}
{'loss': 1.7124, 'grad_norm': 13.369233131408691, 'learning_rate': 3.1938278289117485e-08, 'epoch': 0.81}
{'train_runtime': 218.3002, 'train_samples_per_second': 270.669, 'train_steps_per_second': 8.461, 'train_loss': 1.7831050127212202, 'epoch': 1.0}
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1847/1847 [03:38<00:00,  8.46it/s]
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 7386/7386 [01:47<00:00, 68.53it/s]
PredictionOutput(predictions=array([[ 0.88052315,  0.20732781, -1.435508  ],
       [-0.03002564,  1.0943656 , -1.49137   ],
       [ 1.590506  , -0.28230697, -1.7463896 ],
       ...,
       [ 0.8851918 ,  0.7050387 , -2.219893  ],
       [-3.0769954 ,  1.1885352 ,  1.875024  ],
       [-0.5592181 ,  0.99421525, -0.6573812 ]], dtype=float32), label_ids=array([1, 0, 1, ..., 1, 1, 0], dtype=int64), metrics={'test_loss': 1.863357663154602, 'test_accuracy': 0.052109602838754654, 'test_runtime': 107.8018, 'test_samples_per_second': 548.108, 'test_steps_per_second': 68.515})
wrong_indices: [    0     1     2 ... 59084 59085 59086]
len(wrong_indices): 56008
type(wrong_indices): <class 'numpy.ndarray'>
type(train_dataset): <class 'datasets.arrow_dataset.Dataset'>
PS C:\Users\kenta\Desktop\NLP\fp-dataset-artifcats-main_2>
>>

python run.py --do_eval --per_device_train_batch_size 32 --task nli --dataset ./breaking_nli_modified.jsonl --model ./trained_model_epochs_3_wrongex_epoch_1_lr_17e-7 --output_dir  ./eval_trained_model_epochs_3_wrongex_epoch_1_lr_17e-7/

{"eval_loss": 0.16768071055412292, "eval_model_preparation_time": 0.002, "eval_accuracy": 0.9504454731941223, "eval_runtime": 15.1307, "eval_samples_per_second": 541.481, "eval_steps_per_second": 67.743}

python run.py --do_train --per_device_train_batch_size 32 --task nli --dataset ./multinli_1.0_train_formatted_wrong_examples_.jsonl --model ./trained_SNLI_model_epochs_3_MNLI_epochs_3 --output_dir ./trained_model_epochs_3_wrongex_epoch_1_lr_16e-7  --num_train_epochs 1.0 --learning_rate 1.6e-7

PS C:\Users\kenta\Desktop\NLP\fp-dataset-artifcats-main_2> python run.py --do_train --per_device_train_batch_size 32 --task nli --dataset ./multinli_1.0_train_formatted_wrong_examples_.jsonl --model ./trained_SNLI_model_epochs_3_MNLI_epochs_3 --output_dir ./trained_model_epochs_3_wrongex_epoch_1_lr_16e-7  --num_train_epochs 1.0 --learning_rate 1.6e-7
>>
Preprocessing data... (this takes a little bit, should only happen once per dataset)
C:\Users\kenta\Desktop\NLP\fp-dataset-artifcats-main_2\run.py:171: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = trainer_class(
{'loss': 1.9294, 'grad_norm': 16.13016128540039, 'learning_rate': 1.1668651867893883e-07, 'epoch': 0.27}
{'loss': 1.8008, 'grad_norm': 14.190762519836426, 'learning_rate': 7.337303735787763e-08, 'epoch': 0.54}
{'loss': 1.7295, 'grad_norm': 13.549077987670898, 'learning_rate': 3.0059556036816456e-08, 'epoch': 0.81}
{'train_runtime': 218.3005, 'train_samples_per_second': 270.668, 'train_steps_per_second': 8.461, 'train_loss': 1.7963261462316424, 'epoch': 1.0}
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1847/1847 [03:38<00:00,  8.46it/s]
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 7386/7386 [01:47<00:00, 68.68it/s]
PredictionOutput(predictions=array([[ 0.9140276 ,  0.2141058 , -1.4913071 ],
       [-0.02902731,  1.1245365 , -1.5358438 ],
       [ 1.6364604 , -0.295484  , -1.789978  ],
       ...,
       [ 0.89742297,  0.7103485 , -2.2451825 ],
       [-3.1071239 ,  1.1957964 ,  1.8979914 ],
       [-0.5750457 ,  1.020143  , -0.67379016]], dtype=float32), label_ids=array([1, 0, 1, ..., 1, 1, 0], dtype=int64), metrics={'test_loss': 1.8864418268203735, 'test_accuracy': 0.049198638647794724, 'test_runtime': 107.5654, 'test_samples_per_second': 549.312, 'test_steps_per_second': 68.665})
wrong_indices: [    0     1     2 ... 59084 59085 59086]
len(wrong_indices): 56180
type(wrong_indices): <class 'numpy.ndarray'>
type(train_dataset): <class 'datasets.arrow_dataset.Dataset'>
PS C:\Users\kenta\Desktop\NLP\fp-dataset-artifcats-main_2>
>>

python run.py --do_eval --per_device_train_batch_size 32 --task nli --dataset ./breaking_nli_modified.jsonl --model ./trained_model_epochs_3_wrongex_epoch_1_lr_16e-7 --output_dir  ./eval_trained_model_epochs_3_wrongex_epoch_1_lr_16e-7/

{"eval_loss": 0.1682288646697998, "eval_model_preparation_time": 0.001, "eval_accuracy": 0.9503234624862671, "eval_runtime": 15.0005, "eval_samples_per_second": 546.183, "eval_steps_per_second": 68.331}

python run.py --do_train --per_device_train_batch_size 32 --task nli --dataset ./multinli_1.0_train_formatted_wrong_examples_.jsonl --model ./trained_SNLI_model_epochs_3_MNLI_epochs_3 --output_dir ./trained_model_epochs_3_wrongex_epoch_1_lr_19e-7  --num_train_epochs 1.0 --learning_rate 1.9e-7

PS C:\Users\kenta\Desktop\NLP\fp-dataset-artifcats-main_2> python run.py --do_train --per_device_train_batch_size 32 --task nli --dataset ./multinli_1.0_train_formatted_wrong_examples_.jsonl --model ./trained_SNLI_model_epochs_3_MNLI_epochs_3 --output_dir ./trained_model_epochs_3_wrongex_epoch_1_lr_19e-7  --num_train_epochs 1.0 --learning_rate 1.9e-7
>>
Preprocessing data... (this takes a little bit, should only happen once per dataset)
C:\Users\kenta\Desktop\NLP\fp-dataset-artifcats-main_2\run.py:171: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = trainer_class(
{'loss': 1.9127, 'grad_norm': 16.07852554321289, 'learning_rate': 1.3856524093123985e-07, 'epoch': 0.27}
{'loss': 1.7617, 'grad_norm': 13.544761657714844, 'learning_rate': 8.713048186247969e-08, 'epoch': 0.54}
{'loss': 1.679, 'grad_norm': 13.020187377929688, 'learning_rate': 3.5695722793719544e-08, 'epoch': 0.81}
{'train_runtime': 218.4723, 'train_samples_per_second': 270.455, 'train_steps_per_second': 8.454, 'train_loss': 1.7572519150565105, 'epoch': 1.0}
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1847/1847 [03:38<00:00,  8.45it/s]
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 7386/7386 [01:47<00:00, 68.71it/s]
PredictionOutput(predictions=array([[ 0.81689173,  0.19349791, -1.3282418 ],
       [-0.03133754,  1.0350559 , -1.4049729 ],
       [ 1.4994749 , -0.25671312, -1.6588551 ],
       ...,
       [ 0.86148536,  0.6939969 , -2.1697402 ],
       [-3.0161948 ,  1.1733943 ,  1.8291714 ],
       [-0.5273743 ,  0.9430434 , -0.6258186 ]], dtype=float32), label_ids=array([1, 0, 1, ..., 1, 1, 0], dtype=int64), metrics={'test_loss': 1.818435788154602, 'test_accuracy': 0.05896390229463577, 'test_runtime': 107.5186, 'test_samples_per_second': 549.552, 'test_steps_per_second': 68.695})
wrong_indices: [    0     1     2 ... 59084 59085 59086]
len(wrong_indices): 55603
type(wrong_indices): <class 'numpy.ndarray'>
type(train_dataset): <class 'datasets.arrow_dataset.Dataset'>
PS C:\Users\kenta\Desktop\NLP\fp-dataset-artifcats-main_2>
>>

python run.py --do_eval --per_device_train_batch_size 32 --task nli --dataset ./breaking_nli_modified.jsonl --model ./trained_model_epochs_3_wrongex_epoch_1_lr_19e-7 --output_dir  ./eval_trained_model_epochs_3_wrongex_epoch_1_lr_19e-7/

{"eval_loss": 0.16684547066688538, "eval_model_preparation_time": 0.001, "eval_accuracy": 0.9502013921737671, "eval_runtime": 14.9956, "eval_samples_per_second": 546.359, "eval_steps_per_second": 68.353}

python run.py --do_train --per_device_train_batch_size 32 --task nli --dataset ./multinli_1.0_train_formatted_wrong_examples_.jsonl --model ./trained_SNLI_model_epochs_3_MNLI_epochs_3 --output_dir ./trained_model_epochs_3_wrongex_epoch_1_lr_21e-7  --num_train_epochs 1.0 --learning_rate 2.1e-7

PS C:\Users\kenta\Desktop\NLP\fp-dataset-artifcats-main_2> python run.py --do_train --per_device_train_batch_size 32 --task nli --dataset ./multinli_1.0_train_formatted_wrong_examples_.jsonl --model ./trained_SNLI_model_epochs_3_MNLI_epochs_3 --output_dir ./trained_model_epochs_3_wrongex_epoch_1_lr_21e-7  --num_train_epochs 1.0 --learning_rate 2.1e-7
>>
Preprocessing data... (this takes a little bit, should only happen once per dataset)
C:\Users\kenta\Desktop\NLP\fp-dataset-artifcats-main_2\run.py:171: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = trainer_class(
{'loss': 1.9017, 'grad_norm': 16.04469871520996, 'learning_rate': 1.531510557661072e-07, 'epoch': 0.27}
{'loss': 1.7364, 'grad_norm': 13.13398551940918, 'learning_rate': 9.63021115322144e-08, 'epoch': 0.54}
{'loss': 1.6468, 'grad_norm': 12.685200691223145, 'learning_rate': 3.94531672983216e-08, 'epoch': 0.81}
{'train_runtime': 219.1429, 'train_samples_per_second': 269.628, 'train_steps_per_second': 8.428, 'train_loss': 1.7321901595069709, 'epoch': 1.0}
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1847/1847 [03:39<00:00,  8.43it/s]
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 7386/7386 [01:47<00:00, 68.56it/s]
PredictionOutput(predictions=array([[ 0.757699  ,  0.17955722, -1.227111  ],
       [-0.03188578,  0.97745126, -1.3222283 ],
       [ 1.4099417 , -0.23212156, -1.5713097 ],
       ...,
       [ 0.83870846,  0.6824909 , -2.120194  ],
       [-2.954742  ,  1.1575583 ,  1.7833507 ],
       [-0.49557003,  0.893011  , -0.5958941 ]], dtype=float32), label_ids=array([1, 0, 1, ..., 1, 1, 0], dtype=int64), metrics={'test_loss': 1.775190830230713, 'test_accuracy': 0.06546279042959213, 'test_runtime': 107.7471, 'test_samples_per_second': 548.386, 'test_steps_per_second': 68.549})
wrong_indices: [    0     1     2 ... 59084 59085 59086]
len(wrong_indices): 55219
type(wrong_indices): <class 'numpy.ndarray'>
type(train_dataset): <class 'datasets.arrow_dataset.Dataset'>
PS C:\Users\kenta\Desktop\NLP\fp-dataset-artifcats-main_2>

python run.py --do_eval --per_device_train_batch_size 32 --task nli --dataset ./breaking_nli_modified.jsonl --model ./trained_model_epochs_3_wrongex_epoch_1_lr_21e-7 --output_dir  ./eval_trained_model_epochs_3_wrongex_epoch_1_lr_21e-7/

{"eval_loss": 0.1663723886013031, "eval_model_preparation_time": 0.002, "eval_accuracy": 0.9503234624862671, "eval_runtime": 15.2497, "eval_samples_per_second": 537.258, "eval_steps_per_second": 67.215}

python run.py --do_train --per_device_train_batch_size 32 --task nli --dataset ./multinli_1.0_train_formatted_wrong_examples_.jsonl --model ./trained_SNLI_model_epochs_3_MNLI_epochs_3 --output_dir ./trained_model_epochs_3_wrongex_epoch_1_lr_12e-7  --num_train_epochs 1.0 --learning_rate 1.2e-7

PS C:\Users\kenta\Desktop\NLP\fp-dataset-artifcats-main_2> python run.py --do_train --per_device_train_batch_size 32 --task nli --dataset ./multinli_1.0_train_formatted_wrong_examples_.jsonl --model ./trained_SNLI_model_epochs_3_MNLI_epochs_3 --output_dir ./trained_model_epochs_3_wrongex_epoch_1_lr_12e-7  --num_train_epochs 1.0 --learning_rate 1.2e-7
>>
Preprocessing data... (this takes a little bit, should only happen once per dataset)
C:\Users\kenta\Desktop\NLP\fp-dataset-artifcats-main_2\run.py:171: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = trainer_class(
{'loss': 1.9519, 'grad_norm': 16.225183486938477, 'learning_rate': 8.75148890092041e-08, 'epoch': 0.27}
{'loss': 1.855, 'grad_norm': 15.099206924438477, 'learning_rate': 5.502977801840822e-08, 'epoch': 0.54}
{'loss': 1.8008, 'grad_norm': 14.303593635559082, 'learning_rate': 2.2544667027612342e-08, 'epoch': 0.81}
{'train_runtime': 218.0931, 'train_samples_per_second': 270.926, 'train_steps_per_second': 8.469, 'train_loss': 1.8511243819157757, 'epoch': 1.0}
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1847/1847 [03:38<00:00,  8.47it/s]
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 7386/7386 [01:46<00:00, 69.09it/s]
PredictionOutput(predictions=array([[ 1.0590509 ,  0.2394211 , -1.7269682 ],
       [-0.02223979,  1.2474729 , -1.7212824 ],
       [ 1.8219411 , -0.35074943, -1.9614177 ],
       ...,
       [ 0.949175  ,  0.72976357, -2.3475544 ],
       [-3.2254734 ,  1.2224697 ,  1.9901963 ],
       [-0.6367285 ,  1.1252569 , -0.7435913 ]], dtype=float32), label_ids=array([1, 0, 1, ..., 1, 1, 0], dtype=int64), metrics={'test_loss': 1.982813835144043, 'test_accuracy': 0.035896219313144684, 'test_runtime': 106.9185, 'test_samples_per_second': 552.636, 'test_steps_per_second': 69.081})
wrong_indices: [    0     1     2 ... 59084 59085 59086]
len(wrong_indices): 56966
type(wrong_indices): <class 'numpy.ndarray'>
type(train_dataset): <class 'datasets.arrow_dataset.Dataset'>
PS C:\Users\kenta\Desktop\NLP\fp-dataset-artifcats-main_2>
>>

python run.py --do_eval --per_device_train_batch_size 32 --task nli --dataset ./breaking_nli_modified.jsonl --model ./trained_model_epochs_3_wrongex_epoch_1_lr_12e-7 --output_dir  ./eval_trained_model_epochs_3_wrongex_epoch_1_lr_12e-7/

{"eval_loss": 0.1712425798177719, "eval_model_preparation_time": 0.001, "eval_accuracy": 0.9504454731941223, "eval_runtime": 15.0178, "eval_samples_per_second": 545.552, "eval_steps_per_second": 68.252}

python run.py --do_train --per_device_train_batch_size 32 --task nli --dataset ./multinli_1.0_train_formatted_wrong_examples_.jsonl --model ./trained_SNLI_model_epochs_3_MNLI_epochs_3 --output_dir ./trained_model_epochs_3_wrongex_epoch_1_lr_11e-7  --num_train_epochs 1.0 --learning_rate 1.1e-7

PS C:\Users\kenta\Desktop\NLP\fp-dataset-artifcats-main_2> python run.py --do_train --per_device_train_batch_size 32 --task nli --dataset ./multinli_1.0_train_formatted_wrong_examples_.jsonl --model ./trained_SNLI_model_epochs_3_MNLI_epochs_3 --output_dir ./trained_model_epochs_3_wrongex_epoch_1_lr_11e-7  --num_train_epochs 1.0 --learning_rate 1.1e-7
>>
Preprocessing data... (this takes a little bit, should only happen once per dataset)
C:\Users\kenta\Desktop\NLP\fp-dataset-artifcats-main_2\run.py:171: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = trainer_class(
{'loss': 1.9577, 'grad_norm': 16.257524490356445, 'learning_rate': 8.022198159177044e-08, 'epoch': 0.27}
{'loss': 1.8689, 'grad_norm': 15.332623481750488, 'learning_rate': 5.044396318354088e-08, 'epoch': 0.54}
{'loss': 1.8193, 'grad_norm': 14.501092910766602, 'learning_rate': 2.0665944775311316e-08, 'epoch': 0.81}
{'train_runtime': 217.9545, 'train_samples_per_second': 271.098, 'train_steps_per_second': 8.474, 'train_loss': 1.8653065134656708, 'epoch': 1.0}
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1847/1847 [03:37<00:00,  8.47it/s]
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 7386/7386 [01:48<00:00, 68.34it/s]
PredictionOutput(predictions=array([[ 1.09801   ,  0.24505424, -1.7884911 ],
       [-0.01972174,  1.2784823 , -1.7692872 ],
       [ 1.8684864 , -0.3652219 , -2.0032113 ],
       ...,
       [ 0.9629102 ,  0.73406965, -2.373435  ],
       [-3.2544973 ,  1.228464  ,  2.0134099 ],
       [-0.65153646,  1.1517091 , -0.7620833 ]], dtype=float32), label_ids=array([1, 0, 1, ..., 1, 1, 0], dtype=int64), metrics={'test_loss': 2.007906436920166, 'test_accuracy': 0.032731395214796066, 'test_runtime': 108.0933, 'test_samples_per_second': 546.63, 'test_steps_per_second': 68.33})
wrong_indices: [    0     1     2 ... 59084 59085 59086]
len(wrong_indices): 57153
type(wrong_indices): <class 'numpy.ndarray'>
type(train_dataset): <class 'datasets.arrow_dataset.Dataset'>
PS C:\Users\kenta\Desktop\NLP\fp-dataset-artifcats-main_2>

python run.py --do_eval --per_device_train_batch_size 32 --task nli --dataset ./breaking_nli_modified.jsonl --model ./trained_model_epochs_3_wrongex_epoch_1_lr_11e-7 --output_dir  ./eval_trained_model_epochs_3_wrongex_epoch_1_lr_11e-7/


{"eval_loss": 0.17218877375125885, "eval_model_preparation_time": 0.002, "eval_accuracy": 0.9502013921737671, "eval_runtime": 14.9976, "eval_samples_per_second": 546.287, "eval_steps_per_second": 68.344}

python run.py --do_train --per_device_train_batch_size 32 --task nli --dataset ./multinli_1.0_train_formatted_wrong_examples_.jsonl --model ./trained_SNLI_model_epochs_3_MNLI_epochs_3 --output_dir ./trained_model_epochs_3_wrongex_epoch_1_lr_9e-8  --num_train_epochs 1.0 --learning_rate 9e-8

PS C:\Users\kenta\Desktop\NLP\fp-dataset-artifcats-main_2> python run.py --do_train --per_device_train_batch_size 32 --task nli --dataset ./multinli_1.0_train_formatted_wrong_examples_.jsonl --model ./trained_SNLI_model_epochs_3_MNLI_epochs_3 --output_dir ./trained_model_epochs_3_wrongex_epoch_1_lr_9e-8  --num_train_epochs 1.0 --learning_rate 9e-8
>>
Preprocessing data... (this takes a little bit, should only happen once per dataset)
C:\Users\kenta\Desktop\NLP\fp-dataset-artifcats-main_2\run.py:171: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = trainer_class(
{'loss': 1.9692, 'grad_norm': 16.335678100585938, 'learning_rate': 6.563616675690308e-08, 'epoch': 0.27}
{'loss': 1.8971, 'grad_norm': 15.803107261657715, 'learning_rate': 4.1272333513806167e-08, 'epoch': 0.54}
{'loss': 1.8572, 'grad_norm': 14.906100273132324, 'learning_rate': 1.6908500270709257e-08, 'epoch': 0.81}
{'train_runtime': 217.9196, 'train_samples_per_second': 271.141, 'train_steps_per_second': 8.476, 'train_loss': 1.8942213817486973, 'epoch': 1.0}
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1847/1847 [03:37<00:00,  8.48it/s]
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 7386/7386 [01:46<00:00, 69.08it/s]
PredictionOutput(predictions=array([[ 1.1789985 ,  0.2550902 , -1.9137348 ],
       [-0.01350185,  1.3403319 , -1.8668275 ],
       [ 1.9614314 , -0.3949809 , -2.0850253 ],
       ...,
       [ 0.99144876,  0.7418637 , -2.4254541 ],
       [-3.3117363 ,  1.2394494 ,  2.060137  ],
       [-0.6800413 ,  1.2044753 , -0.80028504]], dtype=float32), label_ids=array([1, 0, 1, ..., 1, 1, 0], dtype=int64), metrics={'test_loss': 2.059239387512207, 'test_accuracy': 0.026300201192498207, 'test_runtime': 106.9377, 'test_samples_per_second': 552.537, 'test_steps_per_second': 69.068})
wrong_indices: [    0     1     2 ... 59084 59085 59086]
len(wrong_indices): 57533
type(wrong_indices): <class 'numpy.ndarray'>
type(train_dataset): <class 'datasets.arrow_dataset.Dataset'>
PS C:\Users\kenta\Desktop\NLP\fp-dataset-artifcats-main_2>

python run.py --do_eval --per_device_train_batch_size 32 --task nli --dataset ./breaking_nli_modified.jsonl --model ./trained_model_epochs_3_wrongex_epoch_1_lr_9e-8 --output_dir  ./eval_trained_model_epochs_3_wrongex_epoch_1_lr_9e-8/

{"eval_loss": 0.17429935932159424, "eval_model_preparation_time": 0.002, "eval_accuracy": 0.9500793218612671, "eval_runtime": 14.2179, "eval_samples_per_second": 576.244, "eval_steps_per_second": 72.092}

python run.py --do_train --per_device_train_batch_size 32 --task nli --dataset ./multinli_1.0_train_formatted_wrong_examples_.jsonl --model ./trained_SNLI_model_epochs_3_MNLI_epochs_3 --output_dir ./trained_model_epochs_3_wrongex_epoch_1_lr_8e-8  --num_train_epochs 1.0 --learning_rate 8e-8

PS C:\Users\kenta\Desktop\NLP\fp-dataset-artifcats-main_2> python run.py --do_eval --per_device_train_batch_size 32 --task nli --dataset ./breaking_nli_modified.jsonl --model ./trained_model_epochs_3_wrongex_epoch_1_lr_9e-8 --output_dir  ./eval_trained_model_epochs_3_wrongex_epoch_1_lr_9e-8/
>>
Preprocessing data... (this takes a little bit, should only happen once per dataset)
Map (num_proc=2): 100%|██████████████████████████████████████████████████████████████████████████████████████████| 8193/8193 [00:06<00:00, 1299.78 examples/s]
C:\Users\kenta\Desktop\NLP\fp-dataset-artifcats-main_2\run.py:171: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = trainer_class(
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1025/1025 [00:14<00:00, 72.94it/s]
Evaluation results:
{'eval_loss': 0.17429935932159424, 'eval_model_preparation_time': 0.002, 'eval_accuracy': 0.9500793218612671, 'eval_runtime': 14.2179, 'eval_samples_per_second': 576.244, 'eval_steps_per_second': 72.092}
PS C:\Users\kenta\Desktop\NLP\fp-dataset-artifcats-main_2> python run.py --do_train --per_device_train_batch_size 32 --task nli --dataset ./multinli_1.0_train_formatted_wrong_examples_.jsonl --model ./trained_SNLI_model_epochs_3_MNLI_epochs_3 --output_dir ./trained_model_epochs_3_wrongex_epoch_1_lr_8e-8  --num_train_epochs 1.0 --learning_rate 8e-8
>>
Preprocessing data... (this takes a little bit, should only happen once per dataset)
C:\Users\kenta\Desktop\NLP\fp-dataset-artifcats-main_2\run.py:171: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = trainer_class(
{'loss': 1.975, 'grad_norm': 16.382043838500977, 'learning_rate': 5.834325933946941e-08, 'epoch': 0.27}
{'loss': 1.9114, 'grad_norm': 16.038780212402344, 'learning_rate': 3.668651867893882e-08, 'epoch': 0.54}
{'loss': 1.8765, 'grad_norm': 15.113101959228516, 'learning_rate': 1.5029778018408228e-08, 'epoch': 0.81}
{'train_runtime': 216.9935, 'train_samples_per_second': 272.298, 'train_steps_per_second': 8.512, 'train_loss': 1.9089351172826712, 'epoch': 1.0}
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1847/1847 [03:36<00:00,  8.51it/s]
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 7386/7386 [01:47<00:00, 68.40it/s]
PredictionOutput(predictions=array([[ 1.2209725 ,  0.25934896, -1.9771425 ],
       [-0.00972735,  1.3709923 , -1.9162047 ],
       [ 2.0077236 , -0.41025788, -2.1249223 ],
       ...,
       [ 1.006291  ,  0.7452975 , -2.4515665 ],
       [-3.3399014 ,  1.2443888 ,  2.083666  ],
       [-0.69360846,  1.2306716 , -0.81998533]], dtype=float32), label_ids=array([1, 0, 1, ..., 1, 1, 0], dtype=int64), metrics={'test_loss': 2.0854527950286865, 'test_accuracy': 0.022898437455296516, 'test_runtime': 108.0, 'test_samples_per_second': 547.102, 'test_steps_per_second': 68.389})
wrong_indices: [    0     1     2 ... 59084 59085 59086]
len(wrong_indices): 57734
type(wrong_indices): <class 'numpy.ndarray'>
type(train_dataset): <class 'datasets.arrow_dataset.Dataset'>
PS C:\Users\kenta\Desktop\NLP\fp-dataset-artifcats-main_2>

python run.py --do_eval --per_device_train_batch_size 32 --task nli --dataset ./breaking_nli_modified.jsonl --model ./trained_model_epochs_3_wrongex_epoch_1_lr_8e-8 --output_dir  ./eval_trained_model_epochs_3_wrongex_epoch_1_lr_8e-8/

{"eval_loss": 0.17545922100543976, "eval_model_preparation_time": 0.001, "eval_accuracy": 0.9497131705284119, "eval_runtime": 14.9493, "eval_samples_per_second": 548.054, "eval_steps_per_second": 68.565}

python run.py --do_train --per_device_train_batch_size 32 --task nli --dataset ./multinli_1.0_train_formatted_wrong_examples_.jsonl --model ./trained_SNLI_model_epochs_3_MNLI_epochs_3 --output_dir ./trained_model_epochs_3_wrongex_epoch_1_lr_23e-7 --num_train_epochs 1.0 --learning_rate 2.3e-7

PS C:\Users\kenta\Desktop\NLP\fp-dataset-artifcats-main_2> python run.py --do_train --per_device_train_batch_size 32 --task nli --dataset ./multinli_1.0_train_formatted_wrong_examples_.jsonl --model ./trained_SNLI_model_epochs_3_MNLI_epochs_3 --output_dir ./trained_model_epochs_3_wrongex_epoch_1_lr_23e-7 --num_train_epochs 1.0 --learning_rate 2.3e-7
>>
Preprocessing data... (this takes a little bit, should only happen once per dataset)
C:\Users\kenta\Desktop\NLP\fp-dataset-artifcats-main_2\run.py:171: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = trainer_class(
{'loss': 1.8908, 'grad_norm': 16.006296157836914, 'learning_rate': 1.6773687060097455e-07, 'epoch': 0.27}
{'loss': 1.7117, 'grad_norm': 12.739790916442871, 'learning_rate': 1.054737412019491e-07, 'epoch': 0.54}
{'loss': 1.6157, 'grad_norm': 12.364163398742676, 'learning_rate': 4.3210611802923654e-08, 'epoch': 0.81}
{'train_runtime': 217.7097, 'train_samples_per_second': 271.403, 'train_steps_per_second': 8.484, 'train_loss': 1.7079194156169633, 'epoch': 1.0}
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1847/1847 [03:37<00:00,  8.48it/s]
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 7386/7386 [01:47<00:00, 68.58it/s]
PredictionOutput(predictions=array([[ 0.702819  ,  0.16576155, -1.1323866 ],
       [-0.03181135,  0.92181826, -1.2433096 ],
       [ 1.3223264 , -0.20853692, -1.4843494 ],
       ...,
       [ 0.8167629 ,  0.6706323 , -2.071295  ],
       [-2.8926928 ,  1.1411289 ,  1.7374911 ],
       [-0.4640806 ,  0.84434336, -0.5675869 ]], dtype=float32), label_ids=array([1, 0, 1, ..., 1, 1, 0], dtype=int64), metrics={'test_loss': 1.7336369752883911, 'test_accuracy': 0.07175859063863754, 'test_runtime': 107.7192, 'test_samples_per_second': 548.528, 'test_steps_per_second': 68.567})
wrong_indices: [    0     1     2 ... 59084 59085 59086]
len(wrong_indices): 54847
type(wrong_indices): <class 'numpy.ndarray'>
type(train_dataset): <class 'datasets.arrow_dataset.Dataset'>
PS C:\Users\kenta\Desktop\NLP\fp-dataset-artifcats-main_2>
>>

python run.py --do_eval --per_device_train_batch_size 32 --task nli --dataset ./breaking_nli_modified.jsonl --model ./trained_model_epochs_3_wrongex_epoch_1_lr_23e-7 --output_dir  ./eval_trained_model_epochs_3_wrongex_epoch_1_lr_23e-7/

{"eval_loss": 0.16627436876296997, "eval_model_preparation_time": 0.002, "eval_accuracy": 0.9500793218612671, "eval_runtime": 14.9727, "eval_samples_per_second": 547.195, "eval_steps_per_second": 68.458}

python run.py --do_train --per_device_train_batch_size 32 --task nli --dataset ./multinli_1.0_train_formatted_wrong_examples_.jsonl --model ./trained_SNLI_model_epochs_3_MNLI_epochs_3 --output_dir ./trained_model_epochs_3_wrongex_epoch_1_lr_24e-7 --num_train_epochs 1.0 --learning_rate 2.4e-7

PS C:\Users\kenta\Desktop\NLP\fp-dataset-artifcats-main_2> python run.py --do_train --per_device_train_batch_size 32 --task nli --dataset ./multinli_1.0_train_formatted_wrong_examples_.jsonl --model ./trained_SNLI_model_epochs_3_MNLI_epochs_3 --output_dir ./trained_model_epochs_3_wrongex_epoch_1_lr_24e-7 --num_train_epochs 1.0 --learning_rate 2.4e-7
>>
Preprocessing data... (this takes a little bit, should only happen once per dataset)
C:\Users\kenta\Desktop\NLP\fp-dataset-artifcats-main_2\run.py:171: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = trainer_class(
{'loss': 1.8854, 'grad_norm': 15.984162330627441, 'learning_rate': 1.750297780184082e-07, 'epoch': 0.27}
{'loss': 1.6995, 'grad_norm': 12.54896068572998, 'learning_rate': 1.1005955603681644e-07, 'epoch': 0.54}
{'loss': 1.6007, 'grad_norm': 12.208776473999023, 'learning_rate': 4.5089334055224684e-08, 'epoch': 0.81}
{'train_runtime': 217.558, 'train_samples_per_second': 271.592, 'train_steps_per_second': 8.49, 'train_loss': 1.69608517137165, 'epoch': 1.0}
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1847/1847 [03:37<00:00,  8.49it/s]
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 7386/7386 [01:46<00:00, 69.47it/s]
PredictionOutput(predictions=array([[ 0.67696285,  0.1589776 , -1.0874809 ],
       [-0.03158152,  0.8948192 , -1.205333  ],
       [ 1.2793807 , -0.19713375, -1.4412829 ],
       ...,
       [ 0.8060778 ,  0.66460496, -2.047105  ],
       [-2.861462  ,  1.132723  ,  1.7145274 ],
       [-0.448532  ,  0.8205843 , -0.5540303 ]], dtype=float32), label_ids=array([1, 0, 1, ..., 1, 1, 0], dtype=int64), metrics={'test_loss': 1.7135019302368164, 'test_accuracy': 0.07451723515987396, 'test_runtime': 106.3448, 'test_samples_per_second': 555.617, 'test_steps_per_second': 69.453})
wrong_indices: [    0     1     2 ... 59084 59085 59086]
len(wrong_indices): 54684
type(wrong_indices): <class 'numpy.ndarray'>
type(train_dataset): <class 'datasets.arrow_dataset.Dataset'>
PS C:\Users\kenta\Desktop\NLP\fp-dataset-artifcats-main_2>
>>

python run.py --do_eval --per_device_train_batch_size 32 --task nli --dataset ./breaking_nli_modified.jsonl --model ./trained_model_epochs_3_wrongex_epoch_1_lr_24e-7 --output_dir  ./eval_trained_model_epochs_3_wrongex_epoch_1_lr_24e-7/

{"eval_loss": 0.16637228429317474, "eval_model_preparation_time": 0.002, "eval_accuracy": 0.9500793218612671, "eval_runtime": 13.5466, "eval_samples_per_second": 604.803, "eval_steps_per_second": 75.665}

python run.py --do_train --per_device_train_batch_size 32 --task nli --dataset ./multinli_1.0_train_formatted_wrong_examples_.jsonl --model ./trained_SNLI_model_epochs_3_MNLI_epochs_3 --output_dir ./trained_model_epochs_3_wrongex_epoch_1_lr_26e-7 --num_train_epochs 1.0 --learning_rate 2.6e-7

PS C:\Users\kenta\Desktop\NLP\fp-dataset-artifcats-main_2> python run.py --do_train --per_device_train_batch_size 32 --task nli --dataset ./multinli_1.0_train_formatted_wrong_examples_.jsonl --model ./trained_SNLI_model_epochs_3_MNLI_epochs_3 --output_dir ./trained_model_epochs_3_wrongex_epoch_1_lr_26e-7 --num_train_epochs 1.0 --learning_rate 2.6e-7
>>
Preprocessing data... (this takes a little bit, should only happen once per dataset)
C:\Users\kenta\Desktop\NLP\fp-dataset-artifcats-main_2\run.py:171: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = trainer_class(
{'loss': 1.8747, 'grad_norm': 15.931631088256836, 'learning_rate': 1.8961559285327558e-07, 'epoch': 0.27}
{'loss': 1.6757, 'grad_norm': 12.179730415344238, 'learning_rate': 1.1923118570655116e-07, 'epoch': 0.54}
{'loss': 1.5714, 'grad_norm': 11.908257484436035, 'learning_rate': 4.884677855982674e-08, 'epoch': 0.81}
{'train_runtime': 216.8442, 'train_samples_per_second': 272.486, 'train_steps_per_second': 8.518, 'train_loss': 1.6730221106545242, 'epoch': 1.0}
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1847/1847 [03:36<00:00,  8.52it/s]
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 7386/7386 [01:48<00:00, 68.29it/s]
PredictionOutput(predictions=array([[ 0.6282779 ,  0.14576273, -1.002585  ],
       [-0.03077927,  0.8425873 , -1.132462  ],
       [ 1.195455  , -0.17510517, -1.3563539 ],
       ...,
       [ 0.78521967,  0.65244   , -1.999288  ],
       [-2.7986333 ,  1.115574  ,  1.6685253 ],
       [-0.41798073,  0.77434784, -0.528103  ]], dtype=float32), label_ids=array([1, 0, 1, ..., 1, 1, 0], dtype=int64), metrics={'test_loss': 1.6745219230651855, 'test_accuracy': 0.0807114914059639, 'test_runtime': 108.1835, 'test_samples_per_second': 546.174, 'test_steps_per_second': 68.273})
wrong_indices: [    0     1     2 ... 59084 59085 59086]
len(wrong_indices): 54318
type(wrong_indices): <class 'numpy.ndarray'>
type(train_dataset): <class 'datasets.arrow_dataset.Dataset'>
PS C:\Users\kenta\Desktop\NLP\fp-dataset-artifcats-main_2>

python run.py --do_eval --per_device_train_batch_size 32 --task nli --dataset ./breaking_nli_modified.jsonl --model ./trained_model_epochs_3_wrongex_epoch_1_lr_26e-7 --output_dir  ./eval_trained_model_epochs_3_wrongex_epoch_1_lr_26-7/

{"eval_loss": 0.16687238216400146, "eval_model_preparation_time": 0.002, "eval_accuracy": 0.9505675435066223, "eval_runtime": 14.9594, "eval_samples_per_second": 547.683, "eval_steps_per_second": 68.519}

python run.py --do_train --per_device_train_batch_size 32 --task nli --dataset ./multinli_1.0_train_formatted_wrong_examples_.jsonl --model ./trained_SNLI_model_epochs_3_MNLI_epochs_3 --output_dir ./trained_model_epochs_3_wrongex_epoch_1_lr_27e-7 --num_train_epochs 1.0 --learning_rate 2.7e-7

PS C:\Users\kenta\Desktop\NLP\fp-dataset-artifcats-main_2> python run.py --do_train --per_device_train_batch_size 32 --task nli --dataset ./multinli_1.0_train_formatted_wrong_examples_.jsonl --model ./trained_SNLI_model_epochs_3_MNLI_epochs_3 --output_dir ./trained_model_epochs_3_wrongex_epoch_1_lr_27e-7 --num_train_epochs 1.0 --learning_rate 2.7e-7
>>
Preprocessing data... (this takes a little bit, should only happen once per dataset)
C:\Users\kenta\Desktop\NLP\fp-dataset-artifcats-main_2\run.py:171: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = trainer_class(
{'loss': 1.8694, 'grad_norm': 15.900445938110352, 'learning_rate': 1.9690850027070926e-07, 'epoch': 0.27}
{'loss': 1.6641, 'grad_norm': 12.001248359680176, 'learning_rate': 1.238170005414185e-07, 'epoch': 0.54}
{'loss': 1.5573, 'grad_norm': 11.76306438446045, 'learning_rate': 5.072550081212777e-08, 'epoch': 0.81}
{'train_runtime': 216.8341, 'train_samples_per_second': 272.499, 'train_steps_per_second': 8.518, 'train_loss': 1.6617956073721745, 'epoch': 1.0}
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1847/1847 [03:36<00:00,  8.52it/s]
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 7386/7386 [01:46<00:00, 69.66it/s]
PredictionOutput(predictions=array([[ 0.6053968 ,  0.13937722, -0.96258867],
       [-0.03023657,  0.81739634, -1.097582  ],
       [ 1.1545798 , -0.16448699, -1.3146479 ],
       ...,
       [ 0.7750455 ,  0.6463126 , -1.9756765 ],
       [-2.7670593 ,  1.1068519 ,  1.6454855 ],
       [-0.4030452 ,  0.75192213, -0.51572144]], dtype=float32), label_ids=array([1, 0, 1, ..., 1, 1, 0], dtype=int64), metrics={'test_loss': 1.6556838750839233, 'test_accuracy': 0.08362245559692383, 'test_runtime': 106.0482, 'test_samples_per_second': 557.171, 'test_steps_per_second': 69.648})
wrong_indices: [    0     1     2 ... 59084 59085 59086]
len(wrong_indices): 54146
type(wrong_indices): <class 'numpy.ndarray'>
type(train_dataset): <class 'datasets.arrow_dataset.Dataset'>
PS C:\Users\kenta\Desktop\NLP\fp-dataset-artifcats-main_2>

python run.py --do_eval --per_device_train_batch_size 32 --task nli --dataset ./breaking_nli_modified.jsonl --model ./trained_model_epochs_3_wrongex_epoch_1_lr_27e-7 --output_dir  ./eval_trained_model_epochs_3_wrongex_epoch_1_lr_27-7/

{"eval_loss": 0.167278453707695, "eval_model_preparation_time": 0.001, "eval_accuracy": 0.9502013921737671, "eval_runtime": 14.8352, "eval_samples_per_second": 552.268, "eval_steps_per_second": 69.092}

python run.py --do_train --per_device_train_batch_size 32 --task nli --dataset ./multinli_1.0_train_formatted_wrong_examples_.jsonl --model ./trained_SNLI_model_epochs_3_MNLI_epochs_3 --output_dir ./trained_model_epochs_3_wrongex_epoch_1_lr_29e-7 --num_train_epochs 1.0 --learning_rate 2.9e-7

PS C:\Users\kenta\Desktop\NLP\fp-dataset-artifcats-main_2> python run.py --do_train --per_device_train_batch_size 32 --task nli --dataset ./multinli_1.0_train_formatted_wrong_examples_.jsonl --model ./trained_SNLI_model_epochs_3_MNLI_epochs_3 --output_dir ./trained_model_epochs_3_wrongex_epoch_1_lr_29e-7 --num_train_epochs 1.0 --learning_rate 2.9e-7
>>
Preprocessing data... (this takes a little bit, should only happen once per dataset)
C:\Users\kenta\Desktop\NLP\fp-dataset-artifcats-main_2\run.py:171: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = trainer_class(
{'loss': 1.8588, 'grad_norm': 15.826896667480469, 'learning_rate': 2.114943151055766e-07, 'epoch': 0.27}
{'loss': 1.6412, 'grad_norm': 11.655854225158691, 'learning_rate': 1.329886302111532e-07, 'epoch': 0.54}
{'loss': 1.5298, 'grad_norm': 11.482505798339844, 'learning_rate': 5.4482945316729824e-08, 'epoch': 0.81}
{'train_runtime': 218.0344, 'train_samples_per_second': 270.999, 'train_steps_per_second': 8.471, 'train_loss': 1.639940560283568, 'epoch': 1.0}
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1847/1847 [03:38<00:00,  8.47it/s]
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 7386/7386 [01:47<00:00, 68.91it/s]
PredictionOutput(predictions=array([[ 0.5624044 ,  0.12710164, -0.88735455],
       [-0.02891416,  0.76892716, -1.0309426 ],
       [ 1.0752136 , -0.14406228, -1.2330712 ],
       ...,
       [ 0.75512594,  0.63404346, -1.9290409 ],
       [-2.7035556 ,  1.0891225 ,  1.599286  ],
       [-0.37392974,  0.70850986, -0.49209604]], dtype=float32), label_ids=array([1, 0, 1, ..., 1, 1, 0], dtype=int64), metrics={'test_loss': 1.6192909479141235, 'test_accuracy': 0.08951207250356674, 'test_runtime': 107.204, 'test_samples_per_second': 551.164, 'test_steps_per_second': 68.897})
wrong_indices: [    0     1     2 ... 59084 59085 59086]
len(wrong_indices): 53798
type(wrong_indices): <class 'numpy.ndarray'>
type(train_dataset): <class 'datasets.arrow_dataset.Dataset'>
PS C:\Users\kenta\Desktop\NLP\fp-dataset-artifcats-main_2>
>>

python run.py --do_eval --per_device_train_batch_size 32 --task nli --dataset ./breaking_nli_modified.jsonl --model ./trained_model_epochs_3_wrongex_epoch_1_lr_29e-7 --output_dir  ./eval_trained_model_epochs_3_wrongex_epoch_1_lr_29-7/

{"eval_loss": 0.1684161126613617, "eval_model_preparation_time": 0.002, "eval_accuracy": 0.9500793218612671, "eval_runtime": 14.6434, "eval_samples_per_second": 559.503, "eval_steps_per_second": 69.998}

python run.py --do_train --per_device_train_batch_size 32 --task nli --dataset ./multinli_1.0_train_formatted_wrong_examples_.jsonl --model ./trained_SNLI_model_epochs_3_MNLI_epochs_3 --output_dir ./trained_model_epochs_3_wrongex_epoch_1_lr_28e-7 --num_train_epochs 1.0 --learning_rate 2.8e-7

PS C:\Users\kenta\Desktop\NLP\fp-dataset-artifcats-main_2> python run.py --do_train --per_device_train_batch_size 32 --task nli --dataset ./multinli_1.0_train_formatted_wrong_examples_.jsonl --model ./trained_SNLI_model_epochs_3_MNLI_epochs_3 --output_dir ./trained_model_epochs_3_wrongex_epoch_1_lr_28e-7 --num_train_epochs 1.0 --learning_rate 2.8e-7
>>
Preprocessing data... (this takes a little bit, should only happen once per dataset)
C:\Users\kenta\Desktop\NLP\fp-dataset-artifcats-main_2\run.py:171: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = trainer_class(
{'loss': 1.8641, 'grad_norm': 15.865570068359375, 'learning_rate': 2.0420140768814294e-07, 'epoch': 0.27}
{'loss': 1.6526, 'grad_norm': 11.826619148254395, 'learning_rate': 1.2840281537628587e-07, 'epoch': 0.54}
{'loss': 1.5434, 'grad_norm': 11.62114429473877, 'learning_rate': 5.26042230644288e-08, 'epoch': 0.81}
{'train_runtime': 216.9571, 'train_samples_per_second': 272.344, 'train_steps_per_second': 8.513, 'train_loss': 1.6507658243308405, 'epoch': 1.0}
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1847/1847 [03:36<00:00,  8.51it/s]
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 7386/7386 [01:45<00:00, 69.86it/s]
PredictionOutput(predictions=array([[ 0.5834469 ,  0.13314994, -0.9241798 ],
       [-0.02960795,  0.79283434, -1.063739  ],
       [ 1.1144786 , -0.15413798, -1.2735211 ],
       ...,
       [ 0.7650116 ,  0.6401793 , -1.9522538 ],
       [-2.7353551 ,  1.0980312 ,  1.6223979 ],
       [-0.38835543,  0.7299742 , -0.50372356]], dtype=float32), label_ids=array([1, 0, 1, ..., 1, 1, 0], dtype=int64), metrics={'test_loss': 1.637270212173462, 'test_accuracy': 0.08673650771379471, 'test_runtime': 105.7524, 'test_samples_per_second': 558.73, 'test_steps_per_second': 69.842})
wrong_indices: [    0     1     2 ... 59084 59085 59086]
len(wrong_indices): 53962
type(wrong_indices): <class 'numpy.ndarray'>
type(train_dataset): <class 'datasets.arrow_dataset.Dataset'>
PS C:\Users\kenta\Desktop\NLP\fp-dataset-artifcats-main_2>
>>

python run.py --do_eval --per_device_train_batch_size 32 --task nli --dataset ./breaking_nli_modified.jsonl --model ./trained_model_epochs_3_wrongex_epoch_1_lr_28e-7 --output_dir  ./eval_trained_model_epochs_3_wrongex_epoch_1_lr_28-7/

{"eval_loss": 0.16779282689094543, "eval_model_preparation_time": 0.001, "eval_accuracy": 0.9499572515487671, "eval_runtime": 14.6815, "eval_samples_per_second": 558.05, "eval_steps_per_second": 69.816}


